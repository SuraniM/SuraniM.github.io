@article{MatharaarachchiSurani2022Mfwm,
issn = {2376-5992},
journal = {PeerJ. Computer science},
keywords = {Accuracy ; Algorithms ; Analysis ; Class imbalance ; Classification ; Data mining ; Data Mining and Machine Learning ; Data Science ; Datasets ; Discriminant analysis ; Feature selection ; Machine learning ; Methods ; Principal component loading ; Variance analysis},
language = {eng},
pages = {e1081-e1081},
publisher = {PeerJ. Ltd},
title = {Minimizing features while maintaining performance in data classification problems},
abstract = {High dimensional classification problems have gained increasing attention in machine learning, and feature selection has become essential in executing machine learning algorithms. In general, most feature selection methods compare the scores of several feature subsets and select the one that gives the maximum score. There may be other selections of a lower number of features with a lower score, yet the difference is negligible. This article proposes and applies an extended version of such feature selection methods, which selects a smaller feature subset with similar performance to the original subset under a pre-defined threshold. It further validates the suggested extended version of the Principal Component Loading Feature Selection (PCLFS-ext) results by simulating data for several practical scenarios with different numbers of features and different imbalance rates on several classification methods. Our simulated results show that the proposed method outperforms the original PCLFS and existing Recursive Feature Elimination (RFE) by giving reasonable feature reduction on various data sets, which is important in some applications.},
author = {Matharaarachchi, Surani and Domaratzki, Mike and Muthukumarana, Saman},
address = {San Diego},
copyright = {COPYRIGHT 2022 PeerJ. Ltd.},
volume = {8},
year = {2022},
}

